{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R2-D2 Reranking Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIOgrQtNdhd6",
        "outputId": "0bcb63d8-f79b-48be-bc5c-f07193688815"
      },
      "source": [
        "!wget r2d2.fit.vutbr.cz/checkpoints/nq-open/reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt.zip\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-18 10:18:36--  http://r2d2.fit.vutbr.cz/checkpoints/nq-open/reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt.zip\n",
            "Resolving r2d2.fit.vutbr.cz (r2d2.fit.vutbr.cz)... 147.229.13.167, 2001:67c:1220:80c::93e5:da7\n",
            "Connecting to r2d2.fit.vutbr.cz (r2d2.fit.vutbr.cz)|147.229.13.167|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 462347964 (441M) [application/zip]\n",
            "Saving to: ‘reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt.zip’\n",
            "\n",
            "reranker_roberta-ba 100%[===================>] 440.93M  26.4MB/s    in 45s     \n",
            "\n",
            "2021-04-18 10:19:21 (9.88 MB/s) - ‘reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt.zip’ saved [462347964/462347964]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK9kO0bXfIKs",
        "outputId": "9d57b722-e91f-4aea-93ee-b1e8ca764a45"
      },
      "source": [
        "!unzip reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt.zip\n",
            "  inflating: reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfpCyrAxfoqy"
      },
      "source": [
        "!pip install -q transformers==4.3.3 torchtext==0.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zqf_9BagCIO"
      },
      "source": [
        "import torchtext\n",
        "import torch\n",
        "\n",
        "\n",
        "class RerankerDataset(torchtext.data.Dataset):\n",
        "\n",
        "    def __init__(self, data, query_builder, passages_per_query=1, numerized=False, **kwargs):\n",
        "        self.query_builder = query_builder\n",
        "        self.passages_per_query = passages_per_query\n",
        "        self.numerized = numerized\n",
        "\n",
        "        fields = self.prepare_fields(1.)\n",
        "        examples = self.get_example_list(data, fields)\n",
        "\n",
        "        super().__init__(examples, fields, **kwargs)\n",
        "\n",
        "    def get_example_list(self, data, fields):\n",
        "        question = data[\"question\"]\n",
        "        passages = data[\"passages\"]\n",
        "\n",
        "        if not self.numerized:\n",
        "            question = self.query_builder.tokenize_and_convert_to_ids(question)\n",
        "            passages = [(self.query_builder.tokenize_and_convert_to_ids(item[0]), self.query_builder.tokenize_and_convert_to_ids(item[1])) for item in passages]\n",
        "\n",
        "        max_length = self.query_builder.max_seq_length if self.query_builder.max_seq_length else self.query_builder.tokenizer.model_max_length\n",
        "        max_length-= self.query_builder.num_special_tokens_to_add\n",
        "        max_length-= len(question)\n",
        "\n",
        "        query_length = 0\n",
        "        query_passages = []\n",
        "        examples = []\n",
        "        for (t, p) in passages:\n",
        "\n",
        "            if query_length + len(t) + len(p) + 2 > max_length or len(query_passages) >= self.passages_per_query:\n",
        "                features = self.query_builder(question, query_passages, self.numerized)\n",
        "                examples.append(\n",
        "                    torchtext.data.Example.fromlist(\n",
        "                        [\n",
        "                            features[\"input_ids\"],\n",
        "                            features[\"attention_mask\"],\n",
        "                        ], \n",
        "                        fields\n",
        "                    )\n",
        "                )\n",
        "                query_passages = []\n",
        "                query_length = 0\n",
        "\n",
        "            query_passages.append((t, p))\n",
        "            query_length += len(t) + len(p) + 2\n",
        "\n",
        "        features = self.query_builder(question, query_passages, self.numerized)\n",
        "        examples.append(\n",
        "            torchtext.data.Example.fromlist(\n",
        "                [\n",
        "                    features[\"input_ids\"],\n",
        "                    features[\"attention_mask\"],\n",
        "                ], \n",
        "                fields\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return examples\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_fields(pad_t):\n",
        "        return [\n",
        "            (\"input_ids\", torchtext.data.Field(use_vocab=False, batch_first=True, sequential=True, pad_token=pad_t)),\n",
        "            (\"attention_mask\", torchtext.data.Field(use_vocab=False, batch_first=True, sequential=True, pad_token=0.)),\n",
        "            #(\"score_mask\", torchtext.data.Field(use_vocab=False, batch_first=True, sequential=True, pad_token=float(\"-Inf\")))\n",
        "        ]\n",
        "\n",
        "    @classmethod\n",
        "    def download(cls, root, check=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def filter_examples(self, field_names):\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhNO2jFCgNU6"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import traceback\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import transformers\n",
        "import torch\n",
        "import torchtext\n",
        "import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "SEED = 1601640139674    # seed for deterministic shuffle of passages on longformer input\n",
        "\n",
        "\n",
        "class RerankerFramework(object):\n",
        "    \"\"\" Passage reranker trainner \"\"\"\n",
        "    def __init__(self, device, config, train_dataloader=None, val_dataloader=None):\n",
        "        self.LOGGER = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "        self.device = device\n",
        "        self.config = config\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "\n",
        "    def train(self,\n",
        "              model,\n",
        "              save_ckpt=None,\n",
        "              num_epoch=5,\n",
        "              learning_rate=1e-5,\n",
        "              batch_size=1,\n",
        "              iter_size=16,\n",
        "              warmup_proportion=0.1,\n",
        "              weight_decay_rate=0.01,\n",
        "              no_decay=['bias', 'gamma', 'beta', 'LayerNorm.weight'],\n",
        "              fp16=False,\n",
        "              criterion=None\n",
        "            ):\n",
        "        # Add trainig configuration       \n",
        "        self.config[\"training\"] = {}\n",
        "        self.config[\"training\"][\"num_epoch\"] = num_epoch\n",
        "        self.config[\"training\"][\"lr\"] = learning_rate\n",
        "        self.config[\"training\"][\"train_batch_size\"] = batch_size\n",
        "        self.config[\"training\"][\"iter_size\"] = iter_size\n",
        "        self.config[\"training\"][\"warmup_proportion\"] = warmup_proportion\n",
        "        self.config[\"training\"][\"weight_decay_rate\"] = weight_decay_rate\n",
        "        self.config[\"training\"][\"no_decay\"] = no_decay\n",
        "        self.config[\"training\"][\"fp16\"] = fp16\n",
        "        self.config[\"training\"][\"criterion\"] = criterion\n",
        "\n",
        "        self.LOGGER.info(\"Start training...\")\n",
        "\n",
        "        param_optimizer = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': weight_decay_rate},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "        optimizer = transformers.AdamW(\n",
        "            optimizer_grouped_parameters, \n",
        "            lr=learning_rate,\n",
        "            correct_bias=False\n",
        "        )\n",
        "\n",
        "        if not criterion:\n",
        "            criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        num_training_steps = int(len(self.train_dataloader.dataset) / (iter_size) * num_epoch)\n",
        "        num_warmup_steps = int(num_training_steps * warmup_proportion)\n",
        "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "            optimizer, \n",
        "            num_warmup_steps=num_warmup_steps, \n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "        try:\n",
        "            self.best_val_accuracy = -math.inf\n",
        "            for epoch in range(1, num_epoch+1):\n",
        "                LOGGER.info(f\"Epoch {epoch} started.\")\n",
        "\n",
        "                self.train_epoch(model, optimizer, scheduler, criterion, epoch, iter_size, batch_size, fp16, save_ckpt)\n",
        "\n",
        "            metrics = self.validate(model, self.val_dataloader, criterion)\n",
        "\n",
        "            for key, value in metrics.items():\n",
        "                LOGGER.info(\"Validation after '%i' iterations.\", self.iter)\n",
        "                LOGGER.info(f\"{key}: {value:.4f}\")\n",
        "\n",
        "            if metrics[\"HIT@25\"] > self.best_val_accuracy:\n",
        "                LOGGER.info(\"Best checkpoint.\")\n",
        "                self.best_val_accuracy = metrics[\"HIT@25\"]\n",
        "\n",
        "            if save_ckpt:\n",
        "                self.save_model(model, self.config, optimizer, scheduler,\n",
        "                                save_ckpt+f\"_HIT@25_{metrics['HIT@25']}.ckpt\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            LOGGER.info('Exit from training early.')\n",
        "        except:\n",
        "            LOGGER.exception(\"An exception was thrown during training: \")\n",
        "        finally:\n",
        "            LOGGER.info('Finished after {:0.2f} minutes.'.format((time.time() - start_time) / 60))\n",
        "\n",
        "    def train_epoch(self, model, optimizer, scheduler, criterion, epoch, \n",
        "                    iter_size, batch_size, fp16, save_ckpt):\n",
        "        model.train()\n",
        "\n",
        "        train_loss = 0\n",
        "        train_right = 0\n",
        "\n",
        "        total_preds = []\n",
        "        total_labels = []\n",
        "\n",
        "        postfix = {\"loss\": 0., \"accuracy\": 0., \"skip\": 0}\n",
        "        iter_ = tqdm.tqdm(enumerate(self.train_dataloader, 1), desc=\"[TRAIN]\", total=len(self.train_dataloader.dataset) // self.train_dataloader.batch_size, postfix=postfix)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for it, batch in iter_:\n",
        "            update = False\n",
        "            try:\n",
        "                data = {key: values.to(self.device) for key, values in batch.items()}\n",
        "                logits = model(data)\n",
        "\n",
        "                loss = criterion(logits, data[\"labels\"]) / iter_size\n",
        "\n",
        "                pred = torch.argmax(logits, dim=1)\n",
        "                right = torch.mean((data[\"labels\"].view(-1) == pred.view(-1)).float(), 0)\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_right += right.item()\n",
        "\n",
        "                postfix.update({\"loss\": \"{:.6f}\".format(train_loss / it), \"accuracy\": train_right / it})\n",
        "                iter_.set_postfix(postfix)\n",
        "\n",
        "                total_preds += list(pred.cpu().numpy())\n",
        "                total_labels += list(data[\"labels\"].cpu().numpy())\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                if it % iter_size == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    update = True\n",
        "                    self.iter += 1\n",
        "\n",
        "                    if self.iter % 5000 == 0:\n",
        "                        metrics = self.validate(model, self.val_dataloader, criterion)\n",
        "\n",
        "                        for key, value in metrics.items():\n",
        "                            LOGGER.info(f\"Validation {key} after {self.iter} iteration: {value:.4f}\")\n",
        "\n",
        "                        if metrics[\"HIT@25\"] > self.best_val_accuracy:\n",
        "                            LOGGER.info(\"Best checkpoint.\")\n",
        "                            self.best_val_accuracy = metrics[\"HIT@25\"]\n",
        "\n",
        "                        if save_ckpt:\n",
        "                            self.save_checkpoint(model, config, optmizer, scheduler,\n",
        "                                                 save_ckpt+f\"_HIT@25_{metrics['HIT@25']}.ckpt\")\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"CUDA out of memory.\" in str(e):\n",
        "                    logging.debug(f\"Allocated memory befor: {torch.cuda.memory_allocated(0)}\")\n",
        "                    torch.cuda.empty_cache()\n",
        "                    logging.debug(f\"Allocated memory after: {torch.cuda.memory_allocated(0)}\")\n",
        "                    logging.error(e)\n",
        "                    tb = traceback.format_exc()\n",
        "                    logging.error(tb)\n",
        "                    postfix[\"skip\"] += 1\n",
        "                    iter_.set_postfix(postfix)\n",
        "                else:\n",
        "                    raise e\n",
        "  \n",
        "        if not update:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "        LOGGER.debug(\"Statistics in train time.\")\n",
        "        LOGGER.debug(\"Histogram of predicted passage: %s\", str(Counter(total_preds)))\n",
        "        LOGGER.debug(\"Histogram of labels: %s\", str(Counter(total_labels)))\n",
        "\n",
        "        LOGGER.info('Epoch is ended, samples: {0:5} | loss: {1:2.6f}, accuracy: {2:3.2f}%'.format(len(self.train_dataloader), train_loss / len(self.train_dataloader), 100 * train_right / len(self.train_dataloader)))\n",
        "        return {\n",
        "            \"accuracy\": train_right / len(self.train_dataloader)\n",
        "        }\n",
        "\n",
        "    def _update_parameters(self, optimizer, scheduler, dataloader, it, iter_size, train_loss, train_right):\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        sys.stdout.write('[TRAIN] step: {0:5}/{1:5} | loss: {2:2.6f}, accuracy: {3:3.2f}%'.format(it//iter_size, len(dataloader.dataset)//iter_size, train_loss / it, 100 * train_right / it) +'\\r')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, model, dataloader, criterion):\n",
        "        model.eval()\n",
        "\n",
        "        hits_k = [1, 2, 5, 10, 25, 50, dataloader.dataset.passages_in_batch]\n",
        "        hits_sum = [0 for _ in hits_k]\n",
        "\n",
        "        iter_ = tqdm.tqdm(enumerate(dataloader, 1), desc=\"[EVAL]\", total=len(dataloader))\n",
        "\n",
        "        for it, data in iter_:\n",
        "            batch = {key: data[key].to(self.device) for key in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "            batch_scores = model(batch)\n",
        "            batch_scores = batch_scores.view(-1)\n",
        "            batch_scores = batch_scores[batch_scores != float(\"-Inf\")]\n",
        "\n",
        "            top_k = batch_scores.shape[0]\n",
        "\n",
        "            top_k_indices = torch.topk(batch_scores, top_k)[1].tolist()\n",
        "\n",
        "            hit_rank = -1\n",
        "            for hit_idx, seq_idx in enumerate(top_k_indices):\n",
        "                if seq_idx in data[\"hits\"]:\n",
        "                    hit_rank = hit_idx\n",
        "                    break\n",
        "\n",
        "            for i, k in enumerate(hits_k):\n",
        "                hits_sum[i]+= 1 if -1 < hit_rank < k else 0\n",
        "\n",
        "        return {\n",
        "            f\"HIT@{key}\": value/it for key, value in zip(hits_k, hits_sum)\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    @torch.no_grad()\n",
        "    def infer_longformer(cls, model, query_builder, question, support, return_top=20, \n",
        "                         max_passage_batch=20, top_k_from_retriever=5, numerized=False, \n",
        "                         batch_size=1, device=None):\n",
        "\n",
        "        model.eval()\n",
        "        if not numerized:\n",
        "            question = query_builder.tokenize_and_convert_to_ids(question)\n",
        "            support = [(query_builder.tokenize_and_convert_to_ids(title), query_builder.tokenize_and_convert_to_ids(context)) for title, context in support]\n",
        "\n",
        "        indeces = list(range(top_k_from_retriever, len(support)))\n",
        "\n",
        "        random.seed(SEED)\n",
        "        random.shuffle(indeces)\n",
        "\n",
        "        support = [support[idx] for idx in indeces]\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        dataset = RerankerDataset({\"question\": question, \"passages\": support},\n",
        "                query_builder, passages_per_query=max_passage_batch, \n",
        "                numerized = True)\n",
        "        data_loader = torchtext.data.BucketIterator(\n",
        "                dataset, batch_size=batch_size, shuffle=False, sort=False, \n",
        "                repeat=False)\n",
        "\n",
        "        for batch in data_loader:\n",
        "            batch = {key: getattr(batch, key).to(device) for key in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "            batch_scores = model(batch)\n",
        "            batch_scores = batch_scores.view(-1)\n",
        "            batch_scores = batch_scores[batch_scores != float(\"-Inf\")]\n",
        "\n",
        "            scores.append(batch_scores)\n",
        "\n",
        "        scores = torch.cat(scores).unsqueeze(0)\n",
        "\n",
        "\n",
        "        top_k_indeces = torch.topk(scores, return_top-top_k_from_retriever)[1][0]\n",
        "        indeces = [indeces[idx] for idx in top_k_indeces]\n",
        "        scores = [scores[0][idx].item() for idx in top_k_indeces]\n",
        "\n",
        "        if top_k_from_retriever > 0:\n",
        "            indeces = list(range(top_k_from_retriever)) + indeces\n",
        "            scores = top_k_from_retriever*[max(scores)] + scores\n",
        "\n",
        "        return {\n",
        "            \"indeces\": indeces,\n",
        "            \"scores\": scores\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def save_model(cls, model, config, path):\n",
        "        LOGGER.info(f\"Save checkpoint '{path}'.\")\n",
        "        dict_to_save = {}\n",
        "        dict_to_save[\"model\"]  = model.state_dict()\n",
        "        dict_to_save[\"config\"] = config\n",
        "\n",
        "        torch.save(dict_to_save, path)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, path, device):\n",
        "        if os.path.isfile(path):\n",
        "            model = torch.load(path, map_location=device)\n",
        "            LOGGER.info(f\"Successfully loaded checkpoint '{path}'\")\n",
        "            return model[\"model\"], model[\"config\"]\n",
        "        else:\n",
        "            raise Exception(f\"No checkpoint found at '{path}'\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBJXMutKgUqy",
        "outputId": "30e5765d-2fd4-4302-fce9-a3e687ec1ffc"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlLjoEYOgfLo"
      },
      "source": [
        "reranker_model = \"reranker_roberta-base_2021-02-25-17-27_athena19_HIT@25_0.8299645997487725.ckpt\"\n",
        "model_state_dict, model_config = RerankerFramework.load_model(reranker_model, device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po-9UlSIgrtC"
      },
      "source": [
        "reranker_type = model_config[\"reranker_model_type\"]\n",
        "config = model_config[\"encoder_config\"]\n",
        "encoder = model_config[\"encoder\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tODYZSudmG1a"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "config_dict = config.to_dict()\n",
        "config_dict['use_cache'] = False\n",
        "config = RobertaConfig.from_dict(config_dict)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyJS_xbAmY0A",
        "outputId": "fa91d8e1-501d-4a9c-a4c6-ba236a7ffc31"
      },
      "source": [
        "config"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"architectures\": [\n",
              "    \"RobertaForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.3.3\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": false,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btz2QHiggwuF"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(encoder)\n",
        "encoder = AutoModel.from_config(config)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBvXFmqqg5MM"
      },
      "source": [
        "class BaselineRerankerQueryBuilder(object):\n",
        "\n",
        "    def __init__(self, tokenizer, max_seq_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.start_context_token_id = self.tokenizer.convert_tokens_to_ids(\"madeupword0000\")\n",
        "        self.start_title_token_id = self.tokenizer.convert_tokens_to_ids(\"madeupword0001\")\n",
        "\n",
        "    def tokenize_and_convert_to_ids(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        return self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    @property\n",
        "    def num_special_tokens_to_add(self):\n",
        "        return self.tokenizer.num_special_tokens_to_add(pair=True)\n",
        "\n",
        "    def __call__(self, question, passages, numerized=False):\n",
        "        if not numerized:\n",
        "            question = self.tokenize_and_convert_to_ids(question)\n",
        "            passages = [(self.tokenize_and_convert_to_ids(item[0]), self.tokenize_and_convert_to_ids(item[1])) for item in passages]\n",
        "\n",
        "        cls = self.tokenizer.convert_tokens_to_ids([self.tokenizer.bos_token])\n",
        "        sep = self.tokenizer.convert_tokens_to_ids([self.tokenizer.sep_token])\n",
        "        eos = self.tokenizer.convert_tokens_to_ids([self.tokenizer.eos_token])\n",
        "\n",
        "        input_ids_list = []\n",
        "\n",
        "        for passage in passages:\n",
        "            input_ids = cls + question + sep + sep\n",
        "            input_ids.extend([self.start_title_token_id] + passage[0])\n",
        "            input_ids.extend([self.start_context_token_id] + passage[1] + eos)\n",
        "\n",
        "            if len(input_ids) > self.max_seq_length:\n",
        "                input_ids = input_ids[:self.max_seq_length-1] + eos\n",
        "\n",
        "            input_ids_list.append(input_ids)\n",
        "    \n",
        "        seq_len = max(map(len, input_ids_list))\n",
        "\n",
        "        input_ids_tensor = torch.ones(len(input_ids_list), seq_len).long()\n",
        "        attention_mask_tensor = torch.zeros(len(input_ids_list), seq_len).long()\n",
        "\n",
        "        for batch_index, input_ids in enumerate(input_ids_list):\n",
        "\n",
        "            for sequence_index, value in enumerate(input_ids):\n",
        "                input_ids_tensor[batch_index][sequence_index] = value\n",
        "                attention_mask_tensor[batch_index][sequence_index] = 1.\n",
        "\n",
        "        features = {\n",
        "            \"input_ids\": input_ids_tensor,\n",
        "            \"attention_mask\": attention_mask_tensor\n",
        "        }\n",
        "\n",
        "        return features"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tf6omFchcMq"
      },
      "source": [
        "query_builder = BaselineRerankerQueryBuilder(tokenizer, model_config[\"max_length\"])\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxVOpet_h-q7"
      },
      "source": [
        "class BaselineReranker(torch.nn.Module):\n",
        "    \"\"\" Baseline passage reranker used in the paper. \"\"\"\n",
        "\n",
        "    def __init__(self, config, encoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.encoder = encoder\n",
        "        self.vt = torch.nn.Linear(config.hidden_size, 1, bias=False)\n",
        "\n",
        "        self.init_weights(type(self.encoder))\n",
        "\n",
        "    def init_weights(self, clz):\n",
        "        \"\"\" Applies model's weight initialization to all non-pretrained parameters of this model\"\"\"\n",
        "        for ch in self.children():\n",
        "            if issubclass(ch.__class__, torch.nn.Module) and not issubclass(ch.__class__, PreTrainedModel):\n",
        "                ch.apply(lambda module: clz._init_weights(self.encoder, module))\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\"\n",
        "        The input looks like:\n",
        "        [CLS] Q [SEP] <t> title <c> context [EOS]\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = {\n",
        "            \"input_ids\": batch[\"input_ids\"],\n",
        "            \"attention_mask\": batch[\"attention_mask\"]\n",
        "        }\n",
        "\n",
        "        outputs = self.encoder(**inputs)[1]\n",
        "\n",
        "        scores = self.vt(outputs)\n",
        "        scores = scores.view(1,-1)\n",
        "\n",
        "        return scores"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LO_gLG2h0U9"
      },
      "source": [
        "from transformers import PreTrainedModel\n",
        "model = BaselineReranker(\n",
        "                config,\n",
        "                encoder)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpyRkg50iAl6",
        "outputId": "91a4ac9c-8c01-4f12-ecf7-301606959514"
      },
      "source": [
        "model.load_state_dict(model_state_dict, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaselineReranker(\n",
              "  (encoder): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (vt): Linear(in_features=768, out_features=1, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CYF-uutieYE"
      },
      "source": [
        "question = \"How is the PM of India?\"\n",
        "passages = [\"I am PM of india\", \"Narendra Modi PM of  india\", \"I love cricket\"]\n",
        "k_top = 3\n",
        "batch_size = 2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYQtVjsAjd32"
      },
      "source": [
        "scores = []\n",
        "for i in range(0, k_top, batch_size):\n",
        "    passages_sublist = passages[i:i+batch_size]\n",
        "    batch = query_builder(question, passages_sublist, False)\n",
        "    batch = {key: batch[key].to(device) for key in [\"input_ids\", \"attention_mask\"]}\n",
        "    batch_scores = model(batch)\n",
        "    batch_scores = batch_scores.view(-1)\n",
        "    scores.append(batch_scores)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke4mxjAtjxVB"
      },
      "source": [
        "scores = torch.cat(scores).unsqueeze(0)\n",
        "top_k_indeces = torch.topk(scores, k_top)[1][0]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjfikEFKmn9H",
        "outputId": "293cf9b9-4036-4cf3-908b-3188c572e8ea"
      },
      "source": [
        "top_k_indeces"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}